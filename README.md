# century
The code in this repo extracts specific clinical variables from some unstructured notes in an Excel file, and evaluates the quality of that extraction. The extraction.py file prompts an LLM (I used my own OpenAI API key) to extract the key features, amd the hallucinations.py file then uses string matching to check if the extracted fields were hallucinated. The main.py file calls the extractor and hallucination checker, and writes the results (along with the original unstructured notes) to a json file (clinical_notes_extracted_json). The judge.py file calls a second, lighter-weight LLM to evalutae the results of the first LLM, and also performs a simple structure check to make sure our extracted features are in the correct JSON format.

I worked on this assignment for 1 hour and 45 minutes (leaving myself 15 min for this write-up). With more time, I would have used Langchain to modularly invoke the LLMs and template my prompts. Given the time-crunch and lack of need for modularity / abstraction, I instead manually called the OpenAI API. With more time I also would have probably tried fine-tuning a custom 'extraction judge' model instead of my simpler out-of-the-box gpt-3.5-turbo. 

A couple design choice notes: I set the temperature of my models to 0 given the extreme cost of hallucinations in the medical industry. I also chose to include the initial unstructured notes in my output json so that the extracted-field json viewers can choose whether or not to rely on my model's extractions, and verify for themselves the fields I flagged for potential hallucinations. Finally, I believe the fields I chose to extract efficiently capture all the necessary data a macro-level researcher or individual healthcare-provider might need: who the patient is, why they're seeing a doctor, what risk-factors they carry, what they score on key biological metrics, and what the doctor recommended. 

I made light use of ChatGPT in my work, mostly to help with syntax around the OpenAI API calls. 
